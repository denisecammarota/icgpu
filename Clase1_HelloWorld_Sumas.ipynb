{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase1_HelloWorld_Sumas",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denisecammarota/icgpu/blob/main/Clase1_HelloWorld_Sumas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6HXDOSw3hc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a753d6-e4cd-4051-caae-0c479f93357c"
      },
      "source": [
        "%%html\n",
        "<marquee style='width: 100%; color: blue;'><b>ICNPG2021 en Google Colaboratory-Instituto Balseiro-Clase 1 </b></marquee>\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<marquee style='width: 100%; color: blue;'><b>ICNPG2021 en Google Colaboratory-Instituto Balseiro-Clase 1 </b></marquee>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH1rrvSR7z7t"
      },
      "source": [
        "# Preparativos para programar CUDA C/C++ en google colabs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8w8rRnK_fDB"
      },
      "source": [
        "Hola!, Bueno, aqui va un ejemplito de como correr codigo CUDA C/C++ en colabs\n",
        "[[1]](https://https://www.wikihow.com/Run-CUDA-C-or-C%2B%2B-on-Jupyter-(Google-Colab).\n",
        "\n",
        "No se olviden de Runtime-> Change Runtime Type -> GPU. Para que funque a cada linea la tienen que ejecutar con un Shift-Enter o Ctrl-Enter o el botoncito de play."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snj3bwGx_z-c"
      },
      "source": [
        "miremos que version de nvcc tenemos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzIRSGebudXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d948d671-97c2-4b59-f6b7-bcd7bec9e556"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBqFrcSgAAYD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjdc7qaxACgq"
      },
      "source": [
        "A ver que placa nos toco..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAOT_KANAFS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fea86b90-01c2-4a2a-a132-9a5e18764784"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Apr  9 20:23:28 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ELijuYTAHWS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI2A2Sz0ARXc"
      },
      "source": [
        "lindas GPUs!!. Ahora, para poder correr Cuda C/C++ instalamos un plugin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qustrOFnAZV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4202c72b-e3e9-44e3-be89-b813408133fd"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-kqk3ix62\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-kqk3ix62\n",
            "Requirement already satisfied (use --upgrade to upgrade): NVCCPlugin==0.0.2 from git+git://github.com/andreinechaev/nvcc4jupyter.git in /usr/local/lib/python3.7/dist-packages\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp37-none-any.whl size=4307 sha256=bd7c4ebd004c5427030e05bc4366593d4ada1c95737c3aa7795fa057d1db7b20\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bg5v9iz6/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FT5VL6VAh0U"
      },
      "source": [
        "y luego:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKpaprRBvkc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a2d7ca2-5eb8-4ea3-f38c-ab188891d146"
      },
      "source": [
        "%load_ext nvcc_plugin\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "directory /content/src already exists\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKtj2kNVLTcg"
      },
      "source": [
        "Listo!, con eso ya podemos correr codigo CUDA C/C++ en el notebook Jupyter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO4n1xfLK9oO"
      },
      "source": [
        "Para terminar, es conveniente montar nuestro google drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbvLlxFh7pZz"
      },
      "source": [
        "# ¡Hola Mundo en GPU!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6U5nz4TAuid"
      },
      "source": [
        "Probemos ahora un programita simple. Notar que al principio tiene que tener un %%cu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO-e9SlIvm6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350fbf45-177e-4e43-ca2b-e6e83d141a77"
      },
      "source": [
        "%%cu\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void helloFromGPU()\n",
        "{\n",
        "    printf(\"Hello World from GPU!\\n\");\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    printf(\"Hello World from CPU!\\n\");\n",
        "\n",
        "    helloFromGPU<<<1, 10>>>(); \n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello World from CPU!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrFrMhzNF2xz"
      },
      "source": [
        "- probamos modificando la cantidad de hilos, cambia cantidad de veces de impresión (o sea, el segundo numerito de helloFromGPU)\n",
        "- esta limitado hasta 1024, se pueden poner hasta 1024 hilos en un solo bloque\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ9vg5ri8MjE"
      },
      "source": [
        "# Grillas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjXYqDbK94GA"
      },
      "source": [
        "Veamos como trabajan los hilos..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_S1bCql9WRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0ee50d-62a2-4997-dfd7-f0fc56bf91ff"
      },
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "/* kernel */\n",
        "__global__ void Quiensoy()\n",
        "{\n",
        "\tprintf(\"Soy el thread (%d,%d,%d) del bloque (%d,%d,%d) [blockDim=(%d,%d,%d),gridDim=(%d,%d,%d)] \\n\",\n",
        "        threadIdx.x,threadIdx.y,threadIdx.z,blockIdx.x,blockIdx.y,blockIdx.z,\n",
        "        blockDim.x,blockDim.y,blockDim.z,gridDim.x,gridDim.y,gridDim.z);\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\t//TODO: pruebe distintas grillas\t\n",
        "\n",
        "\t//ejemplo1: 4 blocks, y 3 threads/block: \n",
        "\tdim3 nb(4,2,1); dim3 nt(3,1,1);\t\t\n",
        "\tQuiensoy<<< nb, nt>>>();\n",
        "\n",
        "\t// espera a que los threads hayan terminado\n",
        "\tcudaDeviceSynchronize();\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo3PhYkBJX0i"
      },
      "source": [
        "- nb: dimensiones de grilla en términos de bloques (x,y,z)\n",
        "- nt: dimensiones de threads o hilos (x,y,z)\n",
        "- dim3 es un struct de CUDA (sino ponemos nada, asume = 1)\n",
        "- 8 bloques, 3 threads o bloques\n",
        "- van a haber 8 x 3 = 24 impresiones\n",
        "- no hay garantía de ningún orden al imprimir, puede terminar un hilo antes que otro y etc\n",
        "- dice soy el 'indice del thread' del bloque 'indice del bloque'\n",
        "- los indices del thread son del 0 al 2, los del bloque son en la grilla bidimensional en (x,y) correspondiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2zHrEFs-hC3"
      },
      "source": [
        "# Suma de vectores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_MnMfnUA46D"
      },
      "source": [
        "Hagamos primero la suma de dos vectores en la CPU, serialmente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hvU9q4z-n0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a606b9-de78-493b-c625-4337a9fb4655"
      },
      "source": [
        "%%cu\n",
        "// solucion en la cpu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "#include <ctime>\n",
        "\n",
        "#define SIZE\t1024\n",
        "#define NVECES\t1\n",
        "\n",
        "void VectorAdd(int *a, int *b, int *c, int n)\n",
        "{\n",
        "\tfor(int i=0;i<n;i++)\n",
        "\t{\n",
        "\t\tc[i] = a[i] + b[i];\n",
        "\t}\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "\tint N;\n",
        "\t\n",
        "\t// argumentos por linea de comandos\n",
        "\tif(argc==2) N=atoi(argv[1]);\n",
        "\telse N=SIZE;\n",
        "\n",
        "\tint *a, *b, *c;\n",
        "\n",
        "\t// alocacion de memoria de host\n",
        "\ta = (int *)malloc(N*sizeof(int));\n",
        "\tb = (int *)malloc(N*sizeof(int));\n",
        "\tc = (int *)malloc(N*sizeof(int));\n",
        "\n",
        "\t// inicializacion\n",
        "\tfor( int i = 0; i < N; ++i )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "\t\tb[i] = i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\n",
        "\t// suma vectores c[i]=a[i]+b[i], i=0,...,N-1\n",
        "\tfor(int i=0;i<NVECES;i++)\n",
        "\tVectorAdd(a, b, c, N);\n",
        "\n",
        "\t// verificacion de resultado\n",
        "\tfor( int i = 0; i < 10; ++i){\n",
        "\t\tprintf(\"c[%d] = %d\\n\", i, c[i]);\n",
        "\t\tassert(c[i]==2*i);\n",
        "\t}\n",
        "\n",
        "\t// liberacion de memoria\n",
        "\tfree(a);\n",
        "\tfree(b);\n",
        "\tfree(c);\n",
        "\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c[0] = 0\n",
            "c[1] = 2\n",
            "c[2] = 4\n",
            "c[3] = 6\n",
            "c[4] = 8\n",
            "c[5] = 10\n",
            "c[6] = 12\n",
            "c[7] = 14\n",
            "c[8] = 16\n",
            "c[9] = 18\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVFtE-zjAvk3"
      },
      "source": [
        "Ahora implementemos una suma de vectores muy simple en GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqcWRucxBL4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429ff3a5-f5f6-4cb8-d6c9-ac752d2a1741"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define SIZE\t1024\n",
        "\n",
        "// kernel\n",
        "__global__ void VectorAdd(int *a, int *b, int *c, int n)\n",
        "{\n",
        "\tint i = threadIdx.x;\n",
        "\n",
        "\tif (i < n)\n",
        "\t\tc[i] = a[i] + b[i];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "\tint N;\n",
        "\t\n",
        "\tif(argc==2) N=atoi(argv[1]);\n",
        "\telse N=SIZE;\n",
        "\n",
        "\t// punteros a memoria de host\n",
        "\tint *a, *b, *c;\n",
        "\n",
        "\t// punteros a memoria de device\n",
        "\tint *d_a, *d_b, *d_c;\n",
        "\n",
        "\t// alocacion memoria de host\n",
        "\ta = (int *)malloc(N*sizeof(int));\n",
        "\tb = (int *)malloc(N*sizeof(int));\n",
        "\tc = (int *)malloc(N*sizeof(int));\n",
        "\n",
        "\t// alocacion memoria de device\n",
        "\tcudaMalloc( &d_a, N*sizeof(int));\n",
        "\tcudaMalloc( &d_b, N*sizeof(int));\n",
        "\tcudaMalloc( &d_c, N*sizeof(int));\n",
        "\n",
        "\t// inicializacion arrays de host\n",
        "\tfor( int i = 0; i < N; ++i )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "\t\tb[i] = i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\t// copia de host a device\n",
        "\tcudaMemcpy( d_a, a, N*sizeof(int), cudaMemcpyHostToDevice );\n",
        "\tcudaMemcpy( d_b, b, N*sizeof(int), cudaMemcpyHostToDevice );\n",
        "\tcudaMemcpy( d_c, c, N*sizeof(int), cudaMemcpyHostToDevice );\n",
        "\n",
        "\t// suma paralela en el device\n",
        "\tVectorAdd<<< 1, N >>>(d_a, d_b, d_c, N);\n",
        "\t\n",
        "\n",
        "\t// copia (solo del resultado) del device a host\n",
        "\tcudaMemcpy( c, d_c, N*sizeof(int), cudaMemcpyDeviceToHost );\n",
        "\n",
        "\t// verificacion del resultado\n",
        "\tfor( int i = 0; i < N; ++i){\n",
        "\t\tprintf(\"c[%d] = %d\\n\", i, c[i]);\n",
        "\t\tassert(c[i]==2*i);\n",
        "\t}\n",
        "\n",
        "\t// liberacion memoria de host\n",
        "\tfree(a);\n",
        "\tfree(b);\n",
        "\tfree(c);\n",
        "\n",
        "\t// liberacion memoria de device\n",
        "\tcudaFree(d_a);\n",
        "\tcudaFree(d_b);\n",
        "\tcudaFree(d_c);\n",
        "\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01ab6b3c-a303-4897-8a5e-97d5e92b10fc.out: /tmp/tmp0hl940ad/01ab6b3c-a303-4897-8a5e-97d5e92b10fc.cu:62: int main(int, char**): Assertion `c[i]==2*i' failed.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w60kvY8CDB2"
      },
      "source": [
        "¿Que pasa en el programa anterior si pongo N>1024 o N<1024?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyvnNP3GCfCB"
      },
      "source": [
        "Tratemos de arreglarlo para N>1024 dandole mas trabajo a cada hilo..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRsmPj5YCqaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b420e81-f84e-46c9-cd79-95623a42850a"
      },
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "#include <stdlib.h>\n",
        "#include \"/content/drive/MyDrive/Classroom/Introducción al Cálculo Numérico en Procesadores Gráficos Materia Optativa/gpu_timer.h\"\n",
        "\n",
        "#define SIZE\t2048\n",
        "\n",
        "// kernel\n",
        "__global__ void VectorAdd(int *a, int *b, int *c, int n)\n",
        "{\n",
        "\tint i = threadIdx.x;\n",
        "\n",
        "\twhile(i<n){\n",
        "\t\tc[i] = a[i] + b[i];\n",
        "\t\ti+=blockDim.x;\n",
        "\t}\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "\tint N;\n",
        "\t\n",
        "\tif(argc==2) N=atoi(argv[1]);\n",
        "\telse N=SIZE;\n",
        "\n",
        "\t// punteros a memoria de host\n",
        "\tint *a, *b, *c;\n",
        "\n",
        "\t// punteros a memoria de device\n",
        "\tint *d_a, *d_b, *d_c;\n",
        "\n",
        "\t// alocacion memoria de host\n",
        "\ta = (int *)malloc(N*sizeof(int));\n",
        "\tb = (int *)malloc(N*sizeof(int));\n",
        "\tc = (int *)malloc(N*sizeof(int));\n",
        "\n",
        "\t// alocacion memoria de device\n",
        "\tcudaMalloc( &d_a, N*sizeof(int));\n",
        "\tcudaMalloc( &d_b, N*sizeof(int));\n",
        "\tcudaMalloc( &d_c, N*sizeof(int));\n",
        "\n",
        "\t// inicializacion arrays de host\n",
        "\tfor( int i = 0; i < N; ++i )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "\t\tb[i] = i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\t// copia de host a device\n",
        "\tcudaMemcpy( d_a, a, N*sizeof(int), cudaMemcpyHostToDevice );\n",
        "\tcudaMemcpy( d_b, b, N*sizeof(int), cudaMemcpyHostToDevice );\n",
        "\tcudaMemcpy( d_c, c, N*sizeof(int), cudaMemcpyHostToDevice );\n",
        "\n",
        "\t// timer para gpu...\n",
        "\tgpu_timer Reloj;\n",
        "\tReloj.tic();\n",
        "\n",
        "\t// suma paralela en el device\n",
        "\tint nthreads_per_block=256;\n",
        "\tassert(nthreads_per_block<=1024);\n",
        "\tVectorAdd<<< 1, nthreads_per_block >>>(d_a, d_b, d_c, N);\n",
        "\t\n",
        "\t// milisegundos transcurridos\n",
        "\tprintf(\"VectorAdd<<< 1, %d>>>, N= %d t= %lf ms\\n\", nthreads_per_block,N, Reloj.tac());\t\n",
        "\n",
        "\t// copia (solo del resultado) del device a host\n",
        "\tcudaMemcpy( c, d_c, N*sizeof(int), cudaMemcpyDeviceToHost );\n",
        "\n",
        "\t// verificacion del resultado\n",
        "\tfor( int i = 0; i < N; ++i){\n",
        "\t\t//printf(\"c[%d] = %d\\n\", i, c[i]);\n",
        "\t\tassert(c[i]==2*i);\n",
        "\t}\n",
        "\n",
        "\t// liberacion memoria de host\n",
        "\tfree(a);\n",
        "\tfree(b);\n",
        "\tfree(c);\n",
        "\n",
        "\t// liberacion memoria de device\n",
        "\tcudaFree(d_a);\n",
        "\tcudaFree(d_b);\n",
        "\tcudaFree(d_c);\n",
        "\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tmp/tmp9oqokz03/3bf76a56-3bc8-4017-a25c-7dfac4a2d4ce.cu:5:10: fatal error: /content/drive/MyDrive/Classroom/Introducción al Cálculo Numérico en Procesadores Gráficos Materia Optativa/gpu_timer.h: No such file or directory\n",
            " #include \"/content/drive/MyDrive/Classroom/Introducción al Cálculo Numérico en Procesadores Gráficos Materia Optativa/gpu_timer.h\"\n",
            "          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "compilation terminated.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7hvSU6MD5uR"
      },
      "source": [
        "Comprobar ahora que pasa para N>1024 y mirar la performance para distintos N."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYs45NK5EMXW"
      },
      "source": [
        "Intentemos otra solución, ahora usando muchos bloques, un hilo por bloque..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgwrEhaEET5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6cbf2b6-b6d8-4243-d61b-71f6408fe25c"
      },
      "source": [
        "%%cu\n",
        "// solucion paralela parchada, correcta pero ineficiente...\n",
        "// nvcc suma_vectores_gpu_muchos_bloques_un_thread_por_bloque.cu -arch=sm_30\n",
        "// si no usamos sm_30 da error a 65536 porque compila para abajo!\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "#include <stdlib.h>\n",
        "#include \"/content/drive/MyDrive/Classroom/Introducción al Cálculo Numérico en Procesadores Gráficos Materia Optativa/gpu_timer.h\"\n",
        "\n",
        "#define SIZE\t1024\n",
        "\n",
        "// kernel\n",
        "__global__ void VectorAdd(int *a, int *b, int *c, int n)\n",
        "{\n",
        "\tint i = blockIdx.x;\n",
        "\n",
        "\tif(i<n){\n",
        "\t\tc[i] = a[i] + b[i];\n",
        "\t}\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "\tint N;\n",
        "\t\n",
        "\tif(argc==2) N=atoi(argv[1]);\n",
        "\telse N=SIZE;\n",
        "\n",
        "\t// punteros a memoria de host\n",
        "\tint *a, *b, *c;\n",
        "\n",
        "\t// punteros a memoria de device\n",
        "\tint *d_a, *d_b, *d_c;\n",
        "\n",
        "\t// alocacion memoria de host\n",
        "\ta = (int *)malloc(N*sizeof(int));\n",
        "\tb = (int *)malloc(N*sizeof(int));\n",
        "\tc = (int *)malloc(N*sizeof(int));\n",
        "\n",
        "\t// alocacion memoria de device\n",
        "\tcudaMalloc( &d_a, N*sizeof(int));\n",
        "\tcudaMalloc( &d_b, N*sizeof(int));\n",
        "\tcudaMalloc( &d_c, N*sizeof(int));\n",
        "\n",
        "\t// inicializacion arrays de host\n",
        "\tfor( int i = 0; i < N; ++i )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "\t\tb[i] = i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\t// copia de host a device\n",
        "\tcudaMemcpy( d_a, a, N*sizeof(int), cudaMemcpyHostToDevice );\n",
        "\tcudaMemcpy( d_b, b, N*sizeof(int), cudaMemcpyHostToDevice );\n",
        "\tcudaMemcpy( d_c, c, N*sizeof(int), cudaMemcpyHostToDevice );\n",
        "\n",
        "\t// timer para gpu...\n",
        "\tgpu_timer Reloj;\n",
        "\tReloj.tic();\n",
        "\n",
        "\t// suma paralela en el device\n",
        "\tint nbloques=N;\n",
        "\tassert(nbloques<2147483647);\n",
        "\tVectorAdd<<< N, 1 >>>(d_a, d_b, d_c, N);\n",
        "\t\n",
        "\t// milisegundos transcurridos\n",
        "\tprintf(\"VectorAdd<<< N, 1 >>>, N= %d t= %lf ms\\n\", N, Reloj.tac());\t\n",
        "\n",
        "\t// copia (solo del resultado) del device a host\n",
        "\tcudaMemcpy( c, d_c, N*sizeof(int), cudaMemcpyDeviceToHost );\n",
        "\n",
        "\t// verificacion del resultado\n",
        "\tfor( int i = 0; i < N; ++i){\n",
        "\t\t//printf(\"c[%d] = %d\\n\", i, c[i]);\n",
        "\t\tassert(c[i]==2*i);\n",
        "\t}\n",
        "\n",
        "\t// liberacion memoria de host\n",
        "\tfree(a);\n",
        "\tfree(b);\n",
        "\tfree(c);\n",
        "\n",
        "\t// liberacion memoria de device\n",
        "\tcudaFree(d_a);\n",
        "\tcudaFree(d_b);\n",
        "\tcudaFree(d_c);\n",
        "\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tmp/tmp52nmpjdu/96632609-2e98-47b7-8211-2d1d7a382d41.cu:7:10: fatal error: /content/drive/MyDrive/Classroom/Introducción al Cálculo Numérico en Procesadores Gráficos Materia Optativa/gpu_timer.h: No such file or directory\n",
            " #include \"/content/drive/MyDrive/Classroom/Introducción al Cálculo Numérico en Procesadores Gráficos Materia Optativa/gpu_timer.h\"\n",
            "          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "compilation terminated.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE0UCFNjEq0j"
      },
      "source": [
        "Probar la performance con distintos N."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX18tcyAE3ZF"
      },
      "source": [
        "Y ahora, la solucion mas general, muchos bloques, muchos hilos por bloque"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxuaEuR_E8wT"
      },
      "source": [
        "%%cu\n",
        "// solucion paralela optima...\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#include \"/content/drive/MyDrive/Classroom/Introducción al Cálculo Numérico en Procesadores Gráficos Materia Optativa/gpu_timer.h\"\n",
        "\n",
        "#define SIZE\t1024\n",
        "\n",
        "// kernel\n",
        "__global__ void VectorAdd(int *a, int *b, int *c, int n)\n",
        "{\n",
        "\t// indice de thread mapeado a indice de array \n",
        "\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tif (i < n)\n",
        "\t\tc[i] = a[i] + b[i];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "\tint N;\n",
        "\t\n",
        "\tif(argc==2) N=atoi(argv[1]);\n",
        "\telse N=SIZE;\n",
        "\n",
        "\t// punteros a memoria de host\n",
        "\tint *a, *b, *c;\n",
        "\n",
        "\t// punteros a memoria de device\n",
        "\tint *d_a, *d_b, *d_c;\n",
        "\n",
        "\t// alocacion memoria de host\n",
        "\ta = (int *)malloc(N*sizeof(int));\n",
        "\tb = (int *)malloc(N*sizeof(int));\n",
        "\tc = (int *)malloc(N*sizeof(int));\n",
        "\n",
        "\t// alocacion memoria de device\n",
        "\tcudaMalloc( &d_a, N*sizeof(int));\n",
        "\tcudaMalloc( &d_b, N*sizeof(int));\n",
        "\tcudaMalloc( &d_c, N*sizeof(int));\n",
        "\n",
        "\t// inicializacion arrays de host\n",
        "\tfor( int i = 0; i < N; ++i )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "\t\tb[i] = i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\t// copia de host a device\n",
        "\tcudaMemcpy( d_a, a, N*sizeof(int), cudaMemcpyHostToDevice );\n",
        "\tcudaMemcpy( d_b, b, N*sizeof(int), cudaMemcpyHostToDevice );\n",
        "\tcudaMemcpy( d_c, c, N*sizeof(int), cudaMemcpyHostToDevice );\n",
        "\n",
        "\t// timer para gpu...\n",
        "\tgpu_timer Reloj;\n",
        "\tReloj.tic();\n",
        "\n",
        "\t// grilla de threads suficientemente grande...\n",
        "\tdim3 nThreads(256);\n",
        "\tdim3 nBlocks((N + nThreads.x - 1) / nThreads.x);\n",
        "\t// suma paralela en el device\n",
        "\tVectorAdd<<< nBlocks, nThreads >>>(d_a, d_b, d_c, N);\n",
        "\t\n",
        "\t// milisegundos transcurridos\n",
        "\tprintf(\"VectorAdd<<< %d, %d >>>, N= %d t= %lf ms\\n\", nBlocks.x, nThreads.x, N, Reloj.tac());\t\n",
        "\n",
        "\t// copia (solo del resultado) del device a host\n",
        "\tcudaMemcpy( c, d_c, N*sizeof(int), cudaMemcpyDeviceToHost );\n",
        "\n",
        "\t// verificacion del resultado\n",
        "\tfor( int i = 0; i < N; ++i){\n",
        "\t\t//printf(\"c[%d] = %d\\n\", i, c[i]);\n",
        "\t\tassert(c[i]==2*i);\n",
        "\t}\n",
        "\n",
        "\t// liberacion memoria de host\n",
        "\tfree(a);\n",
        "\tfree(b);\n",
        "\tfree(c);\n",
        "\n",
        "\t// liberacion memoria de device\n",
        "\tcudaFree(d_a);\n",
        "\tcudaFree(d_b);\n",
        "\tcudaFree(d_c);\n",
        "\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2GbAfVgFIsi"
      },
      "source": [
        "**Tarea:** Comparar la performance de los tres codigos para GPU, con el de CPU, para distintos valores de N."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c7ZHwjPFjTD"
      },
      "source": [
        "Probemos ahora usar la libreria Thrust, que veremos mas adelante, para simplificar la programacion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUey8neRFqEK"
      },
      "source": [
        "%%cu\n",
        "// solucion thrust\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "#include <stdlib.h>\n",
        "#include \"/content/drive/MyDrive/Classroom/Introducción al Cálculo Numérico en Procesadores Gráficos Materia Optativa/gpu_timer.h\"\n",
        "\n",
        "#include <thrust/device_vector.h>\n",
        "#include <thrust/host_vector.h>\n",
        "#include <thrust/sequence.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define SIZE\t1024\n",
        "\n",
        "// no hay kernel!...\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "\tint N;\n",
        "\t\n",
        "\tif(argc==2) N=atoi(argv[1]);\n",
        "\telse N=SIZE;\n",
        "\n",
        "\t// vectores (containers) de device\n",
        "\tthrust::device_vector<int> d_a(N);\n",
        "\tthrust::device_vector<int> d_b(N);\n",
        "\tthrust::device_vector<int> d_c(N);\n",
        "\n",
        "\t// inicializacion arrays de device\n",
        "\tthrust::sequence(d_a.begin(),d_a.end());\n",
        "\tthrust::copy(d_a.begin(),d_a.end(),d_b.begin());\n",
        "\n",
        "\t// timer para gpu...\n",
        "\tgpu_timer Reloj;\n",
        "\tReloj.tic();\n",
        "\n",
        "\t// suma paralela en el device\n",
        "\tusing namespace thrust::placeholders;\n",
        "\tthrust::transform(d_a.begin(), d_a.end(), d_b.begin(), d_c.begin(), _1+_2);\t\n",
        "\t\n",
        "\t// milisegundos transcurridos\n",
        "\tprintf(\"thrust::transform, N= %d t= %lf ms\\n\", N, Reloj.tac());\t\n",
        "\n",
        "\t// crea un vector de host y copia el resultado del device\n",
        "\tthrust::host_vector<int> c(d_c);\n",
        "\n",
        "\t// verificacion del resultado\n",
        "\tfor( int i = 0; i < N; ++i){\n",
        "\t\t//printf(\"c[%d] = %d\\n\", i, c[i]);\n",
        "\t\tassert(c[i]==2*i);\n",
        "\t}\n",
        "\n",
        "\t//la memoria se libera automaticamente...\n",
        "\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}